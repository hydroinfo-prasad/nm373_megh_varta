{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3DIMG_07NOV2019_0000_L1C_SGP.tif', '3DIMG_07NOV2019_0030_L1C_SGP.tif', '3DIMG_07NOV2019_0100_L1C_SGP.tif', '3DIMG_07NOV2019_0130_L1C_SGP.tif', '3DIMG_07NOV2019_0200_L1C_SGP.tif', '3DIMG_07NOV2019_0230_L1C_SGP.tif', '3DIMG_07NOV2019_0300_L1C_SGP.tif', '3DIMG_07NOV2019_0330_L1C_SGP.tif', '3DIMG_07NOV2019_0400_L1C_SGP.tif', '3DIMG_07NOV2019_0430_L1C_SGP.tif', '3DIMG_07NOV2019_0500_L1C_SGP.tif', '3DIMG_07NOV2019_0530_L1C_SGP.tif', '3DIMG_07NOV2019_0600_L1C_SGP.tif', '3DIMG_07NOV2019_0630_L1C_SGP.tif', '3DIMG_07NOV2019_0700_L1C_SGP.tif', '3DIMG_07NOV2019_0730_L1C_SGP.tif', '3DIMG_07NOV2019_0800_L1C_SGP.tif', '3DIMG_07NOV2019_0830_L1C_SGP.tif', '3DIMG_07NOV2019_0859_L1C_SGP.tif', '3DIMG_07NOV2019_0900_L1C_SGP.tif', '3DIMG_07NOV2019_0929_L1C_SGP.tif', '3DIMG_07NOV2019_0930_L1C_SGP.tif', '3DIMG_07NOV2019_0959_L1C_SGP.tif', '3DIMG_07NOV2019_1000_L1C_SGP.tif', '3DIMG_07NOV2019_1030_L1C_SGP.tif', '3DIMG_07NOV2019_1100_L1C_SGP.tif', '3DIMG_07NOV2019_1130_L1C_SGP.tif', '3DIMG_07NOV2019_1200_L1C_SGP.tif', '3DIMG_07NOV2019_1230_L1C_SGP.tif', '3DIMG_07NOV2019_1300_L1C_SGP.tif', '3DIMG_07NOV2019_1330_L1C_SGP.tif', '3DIMG_07NOV2019_1400_L1C_SGP.tif', '3DIMG_07NOV2019_1430_L1C_SGP.tif', '3DIMG_07NOV2019_1500_L1C_SGP.tif', '3DIMG_07NOV2019_1530_L1C_SGP.tif', '3DIMG_07NOV2019_1600_L1C_SGP.tif', '3DIMG_07NOV2019_1630_L1C_SGP.tif', '3DIMG_07NOV2019_2000_L1C_SGP.tif', '3DIMG_07NOV2019_2030_L1C_SGP.tif', '3DIMG_07NOV2019_2100_L1C_SGP.tif', '3DIMG_07NOV2019_2130_L1C_SGP.tif', '3DIMG_07NOV2019_2200_L1C_SGP.tif', '3DIMG_07NOV2019_2230_L1C_SGP.tif', '3DIMG_07NOV2019_2300_L1C_SGP.tif', '3DIMG_07NOV2019_2330_L1C_SGP.tif']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"./INSAT3D_TIR1_India\")\n",
    "files.sort()\n",
    "print(files)\n",
    "tir_to_num = {}\n",
    "for num,file in enumerate(files):\n",
    "    tir_to_num[file] = num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"./INSAT3D_VIS_India\")\n",
    "files.sort()\n",
    "vis_to_num = {}\n",
    "for num,file in enumerate(files):\n",
    "    vis_to_num[file] = num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 12, 'val': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self,file_list,arg, name_to_num = None, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.arg = arg\n",
    "        self.transform = transform\n",
    "        self.name_to_num = name_to_num\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list[0]) if self.arg == 'fusion' else len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.arg == 'fusion':\n",
    "            image1 = np.array(Image.open(f'./INSAT3D_VIS_India/{self.file_list[0][idx]}'))[:,:1072]\n",
    "            image1 = image1.astype(np.float)\n",
    "            \n",
    "            image2 = np.array(Image.open(f'./INSAT3D_TIR1_India/{self.file_list[1][idx]}'))[:,:1072]\n",
    "            image2 = image2.astype(np.float)\n",
    "            \n",
    "            image = np.array([image1,image2])\n",
    "            image = image.astype(np.float)\n",
    "            image = image.transpose( (1,2,0))\n",
    "\n",
    "            mask = np.array(Image.open(f'./../SIH/INSAT_Cloud_Labels/CROPPED_TIFF/CMK_Cropped_{self.name_to_num[self.file_list[0][idx]]}.tif'))[:,:1072]\n",
    "                \n",
    "        elif self.arg == 'vis':\n",
    "            image = np.array(Image.open(f'./INSAT3D_VIS_India/{self.file_list[idx]}'))[:,:1072]\n",
    "            image = image.astype(np.float)\n",
    "            mask = np.array(Image.open(f'./../SIH/INSAT_Cloud_Labels/CROPPED_TIFF/CMK_Cropped_{self.name_to_num[self.file_list[idx]]}.tif'))[:,:1072]\n",
    "        \n",
    "        elif self.arg == 'tir': \n",
    "            image = np.array(Image.open(f'./INSAT3D_TIR1_India/{self.file_list[idx]}'))[:,:1072]\n",
    "            image = image.astype(np.float)\n",
    "            mask = np.array(Image.open(f'./../SIH/INSAT_Cloud_Labels/CROPPED_TIFF/CMK_Cropped_{self.name_to_num[self.file_list[idx]]}.tif'))[:,:1072]\n",
    "\n",
    "        #print(\"image_shape\", image.shape)\n",
    "        #mask_comb = np.logical_or( mask == 1, mask == 3 )\n",
    "        \n",
    "        mask_comb = mask == 1\n",
    "        mask_final = np.array([mask_comb])\n",
    "        mask_final = mask_final.astype(np.float)\n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return [image, mask_final]\n",
    "\n",
    "arg = 'vis' # vis/tir/fusion    \n",
    "    \n",
    "if arg == 'vis':    \n",
    "    files = os.listdir(\"./INSAT3D_VIS_India\")\n",
    "    files.sort()\n",
    "    files = files[8:25]\n",
    "elif arg == 'tir':\n",
    "    files = os.listdir(\"./INSAT3D_TIR1_India\")\n",
    "    files.sort()\n",
    "    files = files[8:25]\n",
    "elif arg == 'fusion':\n",
    "    files1 = os.listdir(\"./INSAT3D_VIS_India\")\n",
    "    files1.sort()\n",
    "    files1 = files1[8:25]\n",
    "\n",
    "    files2 = os.listdir(\"./INSAT3D_TIR1_India\")\n",
    "    files2.sort()\n",
    "    files2 = files2[8:25]\n",
    "\n",
    "    files = (files1,files2)\n",
    "\n",
    "\n",
    "num_files = len(files[0]) if arg=='fusion' else len(files)\n",
    "print(num_files)\n",
    "'''\n",
    "np.random.seed(5)\n",
    "permut = np.random.permutation(num_files)\n",
    "train_index =  permut[:num_files//2] \n",
    "test_index  =  permut[num_files//2:] \n",
    "'''\n",
    "\n",
    "train_index = [1,2,3,5,6,7,9,10,11,13,14,15]\n",
    "test_index = [0,4,8,12,16]\n",
    "\n",
    "\n",
    "if arg == 'fusion':\n",
    "    train_list1 = [ files[0][i] for i in train_index ]\n",
    "    train_list2 = [ files[1][i] for i in train_index ] \n",
    "    train_list = (train_list1, train_list2)    \n",
    "    \n",
    "    test_list1 = [ files[0][i] for i in test_index ]\n",
    "    test_list2 = [ files[1][i] for i in test_index ]\n",
    "    test_list = (test_list1, test_list2)\n",
    "else:\n",
    "    train_list  = [files[i] for i in train_index]\n",
    "    test_list = [files[i] for i in test_index]\n",
    "\n",
    "# use same transform for train/val for this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dic = tir_to_num if arg == 'tir' else vis_to_num\n",
    "\n",
    "train_set = SimDataset(train_list, arg, dic, transform = trans)\n",
    "val_set = SimDataset(test_list, arg, dic, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 984, 1072]) torch.Size([1, 1, 984, 1072])\n",
      "24.0 550.0 127.12196449156656 88.7772273593749\n",
      "0.0 1.0 0.5929498847227278 0.49128435648820695\n"
     ]
    }
   ],
   "source": [
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "for x in [inputs.numpy(), masks.numpy()]:\n",
    "    print(x.min(), x.max(), x.mean(), x.std())\n",
    "\n",
    "#plt.imshow(reverse_transform(inputs[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 64, 984, 1072]             640\n",
      "              ReLU-2        [-1, 64, 984, 1072]               0\n",
      "            Conv2d-3        [-1, 64, 984, 1072]          36,928\n",
      "              ReLU-4        [-1, 64, 984, 1072]               0\n",
      "         MaxPool2d-5         [-1, 64, 492, 536]               0\n",
      "            Conv2d-6        [-1, 128, 492, 536]          73,856\n",
      "              ReLU-7        [-1, 128, 492, 536]               0\n",
      "            Conv2d-8        [-1, 128, 492, 536]         147,584\n",
      "              ReLU-9        [-1, 128, 492, 536]               0\n",
      "        MaxPool2d-10        [-1, 128, 246, 268]               0\n",
      "           Conv2d-11        [-1, 256, 246, 268]         295,168\n",
      "             ReLU-12        [-1, 256, 246, 268]               0\n",
      "           Conv2d-13        [-1, 256, 246, 268]         590,080\n",
      "             ReLU-14        [-1, 256, 246, 268]               0\n",
      "        MaxPool2d-15        [-1, 256, 123, 134]               0\n",
      "           Conv2d-16        [-1, 512, 123, 134]       1,180,160\n",
      "             ReLU-17        [-1, 512, 123, 134]               0\n",
      "           Conv2d-18        [-1, 512, 123, 134]       2,359,808\n",
      "             ReLU-19        [-1, 512, 123, 134]               0\n",
      "         Upsample-20        [-1, 512, 246, 268]               0\n",
      "           Conv2d-21        [-1, 256, 246, 268]       1,769,728\n",
      "             ReLU-22        [-1, 256, 246, 268]               0\n",
      "           Conv2d-23        [-1, 256, 246, 268]         590,080\n",
      "             ReLU-24        [-1, 256, 246, 268]               0\n",
      "         Upsample-25        [-1, 256, 492, 536]               0\n",
      "           Conv2d-26        [-1, 128, 492, 536]         442,496\n",
      "             ReLU-27        [-1, 128, 492, 536]               0\n",
      "           Conv2d-28        [-1, 128, 492, 536]         147,584\n",
      "             ReLU-29        [-1, 128, 492, 536]               0\n",
      "         Upsample-30       [-1, 128, 984, 1072]               0\n",
      "           Conv2d-31        [-1, 64, 984, 1072]         110,656\n",
      "             ReLU-32        [-1, 64, 984, 1072]               0\n",
      "           Conv2d-33        [-1, 64, 984, 1072]          36,928\n",
      "             ReLU-34        [-1, 64, 984, 1072]               0\n",
      "           Conv2d-35         [-1, 1, 984, 1072]              65\n",
      "================================================================\n",
      "Total params: 7,781,761\n",
      "Trainable params: 7,781,761\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.02\n",
      "Forward/backward pass size (MB): 9504.51\n",
      "Params size (MB): 29.69\n",
      "Estimated Total Size (MB): 9538.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_unet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = pytorch_unet.UNet(1)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(1, 984, 1072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from loss import dice_loss\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        \n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                #scheduler.step()\n",
    "                #for param_group in optimizer.param_groups:\n",
    "                #    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device, dtype=torch.float)\n",
    "                labels = labels.to(device, dtype=torch.float)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "            \n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model,optimizer,scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.0005\n",
      "train: bce: 0.753287, dice: 0.304254, loss: 0.528771\n",
      "val: bce: 0.607953, dice: 0.350204, loss: 0.479079\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 1/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.616671, dice: 0.303192, loss: 0.459932\n",
      "val: bce: 0.610278, dice: 0.309541, loss: 0.459909\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 2/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.611662, dice: 0.295675, loss: 0.453668\n",
      "val: bce: 0.629184, dice: 0.292216, loss: 0.460700\n",
      "0m 5s\n",
      "Epoch 3/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.601934, dice: 0.296060, loss: 0.448997\n",
      "val: bce: 0.638509, dice: 0.283458, loss: 0.460983\n",
      "0m 5s\n",
      "Epoch 4/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.583081, dice: 0.287485, loss: 0.435283\n",
      "val: bce: 0.551014, dice: 0.317909, loss: 0.434461\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 5/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.534740, dice: 0.278742, loss: 0.406741\n",
      "val: bce: 0.790090, dice: 0.246639, loss: 0.518364\n",
      "0m 5s\n",
      "Epoch 6/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.616764, dice: 0.273509, loss: 0.445136\n",
      "val: bce: 0.569208, dice: 0.306386, loss: 0.437797\n",
      "0m 5s\n",
      "Epoch 7/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.551200, dice: 0.284274, loss: 0.417737\n",
      "val: bce: 0.552428, dice: 0.272111, loss: 0.412269\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 8/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.536267, dice: 0.264984, loss: 0.400625\n",
      "val: bce: 0.541038, dice: 0.271101, loss: 0.406069\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 9/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.547567, dice: 0.271638, loss: 0.409603\n",
      "val: bce: 0.557549, dice: 0.270033, loss: 0.413791\n",
      "0m 5s\n",
      "Epoch 10/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.557495, dice: 0.272080, loss: 0.414787\n",
      "val: bce: 0.545429, dice: 0.277062, loss: 0.411246\n",
      "0m 5s\n",
      "Epoch 11/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.530309, dice: 0.266292, loss: 0.398301\n",
      "val: bce: 0.555092, dice: 0.255163, loss: 0.405128\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 12/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.529391, dice: 0.262232, loss: 0.395812\n",
      "val: bce: 0.551959, dice: 0.257038, loss: 0.404499\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 13/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.525765, dice: 0.251978, loss: 0.388872\n",
      "val: bce: 0.514747, dice: 0.272502, loss: 0.393625\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 14/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.517144, dice: 0.249883, loss: 0.383513\n",
      "val: bce: 0.520841, dice: 0.281310, loss: 0.401075\n",
      "0m 5s\n",
      "Epoch 15/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.497303, dice: 0.251620, loss: 0.374461\n",
      "val: bce: 0.486475, dice: 0.258169, loss: 0.372322\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 16/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.573234, dice: 0.265032, loss: 0.419133\n",
      "val: bce: 0.562458, dice: 0.297375, loss: 0.429917\n",
      "0m 5s\n",
      "Epoch 17/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.543188, dice: 0.271102, loss: 0.407145\n",
      "val: bce: 0.525941, dice: 0.272665, loss: 0.399303\n",
      "0m 5s\n",
      "Epoch 18/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.523000, dice: 0.257199, loss: 0.390099\n",
      "val: bce: 0.537555, dice: 0.276062, loss: 0.406808\n",
      "0m 5s\n",
      "Epoch 19/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.509611, dice: 0.252607, loss: 0.381109\n",
      "val: bce: 0.517588, dice: 0.249041, loss: 0.383314\n",
      "0m 5s\n",
      "Epoch 20/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.498434, dice: 0.238169, loss: 0.368301\n",
      "val: bce: 0.489362, dice: 0.260080, loss: 0.374721\n",
      "0m 5s\n",
      "Epoch 21/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.487976, dice: 0.237702, loss: 0.362839\n",
      "val: bce: 0.574697, dice: 0.359018, loss: 0.466858\n",
      "0m 5s\n",
      "Epoch 22/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.537454, dice: 0.281086, loss: 0.409270\n",
      "val: bce: 0.534670, dice: 0.242929, loss: 0.388800\n",
      "0m 5s\n",
      "Epoch 23/199\n",
      "----------\n",
      "LR 0.0005\n",
      "train: bce: 0.504251, dice: 0.241166, loss: 0.372709\n",
      "val: bce: 0.496093, dice: 0.249258, loss: 0.372675\n",
      "0m 5s\n",
      "Epoch 24/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.531671, dice: 0.244974, loss: 0.388322\n",
      "val: bce: 0.498967, dice: 0.284675, loss: 0.391821\n",
      "0m 5s\n",
      "Epoch 25/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.493639, dice: 0.252300, loss: 0.372969\n",
      "val: bce: 0.502704, dice: 0.246181, loss: 0.374442\n",
      "0m 5s\n",
      "Epoch 26/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.475689, dice: 0.236225, loss: 0.355957\n",
      "val: bce: 0.487627, dice: 0.263401, loss: 0.375514\n",
      "0m 5s\n",
      "Epoch 27/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.470797, dice: 0.231353, loss: 0.351075\n",
      "val: bce: 0.476929, dice: 0.253509, loss: 0.365219\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 28/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.466780, dice: 0.232558, loss: 0.349669\n",
      "val: bce: 0.471238, dice: 0.239458, loss: 0.355348\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 29/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.473625, dice: 0.228138, loss: 0.350881\n",
      "val: bce: 0.470844, dice: 0.248465, loss: 0.359654\n",
      "0m 5s\n",
      "Epoch 30/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.454934, dice: 0.222276, loss: 0.338605\n",
      "val: bce: 0.521755, dice: 0.251942, loss: 0.386849\n",
      "0m 5s\n",
      "Epoch 31/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.466997, dice: 0.228716, loss: 0.347857\n",
      "val: bce: 0.470309, dice: 0.247394, loss: 0.358852\n",
      "0m 5s\n",
      "Epoch 32/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.438906, dice: 0.214688, loss: 0.326797\n",
      "val: bce: 0.509539, dice: 0.241980, loss: 0.375760\n",
      "0m 5s\n",
      "Epoch 33/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.460508, dice: 0.212303, loss: 0.336406\n",
      "val: bce: 0.500783, dice: 0.247269, loss: 0.374026\n",
      "0m 5s\n",
      "Epoch 34/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.471987, dice: 0.236174, loss: 0.354081\n",
      "val: bce: 0.467388, dice: 0.243647, loss: 0.355518\n",
      "0m 5s\n",
      "Epoch 35/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.448304, dice: 0.221074, loss: 0.334689\n",
      "val: bce: 0.457237, dice: 0.230993, loss: 0.344115\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 36/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.442180, dice: 0.218898, loss: 0.330539\n",
      "val: bce: 0.461880, dice: 0.236353, loss: 0.349116\n",
      "0m 5s\n",
      "Epoch 37/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.452794, dice: 0.206764, loss: 0.329779\n",
      "val: bce: 0.451252, dice: 0.240814, loss: 0.346033\n",
      "0m 5s\n",
      "Epoch 38/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.443627, dice: 0.223337, loss: 0.333482\n",
      "val: bce: 0.481387, dice: 0.243889, loss: 0.362638\n",
      "0m 5s\n",
      "Epoch 39/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.439205, dice: 0.215812, loss: 0.327509\n",
      "val: bce: 0.465057, dice: 0.251054, loss: 0.358056\n",
      "0m 5s\n",
      "Epoch 40/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.430452, dice: 0.210959, loss: 0.320706\n",
      "val: bce: 0.466923, dice: 0.227608, loss: 0.347266\n",
      "0m 5s\n",
      "Epoch 41/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.427160, dice: 0.208212, loss: 0.317686\n",
      "val: bce: 0.466643, dice: 0.220376, loss: 0.343510\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 42/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.418453, dice: 0.203606, loss: 0.311030\n",
      "val: bce: 0.446604, dice: 0.227907, loss: 0.337255\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 43/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.413694, dice: 0.198617, loss: 0.306156\n",
      "val: bce: 0.447617, dice: 0.229594, loss: 0.338605\n",
      "0m 5s\n",
      "Epoch 44/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.409718, dice: 0.201408, loss: 0.305563\n",
      "val: bce: 0.430519, dice: 0.222977, loss: 0.326748\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 45/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.404462, dice: 0.195420, loss: 0.299941\n",
      "val: bce: 0.462648, dice: 0.228224, loss: 0.345436\n",
      "0m 5s\n",
      "Epoch 46/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.390460, dice: 0.190606, loss: 0.290533\n",
      "val: bce: 0.451021, dice: 0.220288, loss: 0.335654\n",
      "0m 5s\n",
      "Epoch 47/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.395965, dice: 0.190939, loss: 0.293452\n",
      "val: bce: 0.456556, dice: 0.229329, loss: 0.342942\n",
      "0m 5s\n",
      "Epoch 48/199\n",
      "----------\n",
      "LR 0.0004\n",
      "train: bce: 0.385995, dice: 0.189141, loss: 0.287568\n",
      "val: bce: 0.469358, dice: 0.219543, loss: 0.344451\n",
      "0m 5s\n",
      "Epoch 49/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.405310, dice: 0.193261, loss: 0.299285\n",
      "val: bce: 0.439300, dice: 0.234267, loss: 0.336783\n",
      "0m 5s\n",
      "Epoch 50/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.393880, dice: 0.197397, loss: 0.295639\n",
      "val: bce: 0.468820, dice: 0.207363, loss: 0.338092\n",
      "0m 5s\n",
      "Epoch 51/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.403725, dice: 0.198970, loss: 0.301347\n",
      "val: bce: 0.457628, dice: 0.214927, loss: 0.336277\n",
      "0m 5s\n",
      "Epoch 52/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.405514, dice: 0.191049, loss: 0.298281\n",
      "val: bce: 0.425450, dice: 0.229389, loss: 0.327420\n",
      "0m 5s\n",
      "Epoch 53/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.375403, dice: 0.186368, loss: 0.280886\n",
      "val: bce: 0.440118, dice: 0.218134, loss: 0.329126\n",
      "0m 5s\n",
      "Epoch 54/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.371174, dice: 0.184690, loss: 0.277932\n",
      "val: bce: 0.424781, dice: 0.210521, loss: 0.317651\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 55/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.0003200000000000001\n",
      "train: bce: 0.366565, dice: 0.175399, loss: 0.270982\n",
      "val: bce: 0.422865, dice: 0.205726, loss: 0.314295\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 56/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.351337, dice: 0.172977, loss: 0.262157\n",
      "val: bce: 0.435185, dice: 0.213171, loss: 0.324178\n",
      "0m 5s\n",
      "Epoch 57/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.352948, dice: 0.174786, loss: 0.263867\n",
      "val: bce: 0.441354, dice: 0.189937, loss: 0.315646\n",
      "0m 5s\n",
      "Epoch 58/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.350792, dice: 0.170289, loss: 0.260540\n",
      "val: bce: 0.424374, dice: 0.189951, loss: 0.307162\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 59/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.343203, dice: 0.166575, loss: 0.254889\n",
      "val: bce: 0.401399, dice: 0.192352, loss: 0.296875\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 60/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.337162, dice: 0.164355, loss: 0.250759\n",
      "val: bce: 0.422884, dice: 0.204692, loss: 0.313788\n",
      "0m 5s\n",
      "Epoch 61/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.333624, dice: 0.164310, loss: 0.248967\n",
      "val: bce: 0.434930, dice: 0.197249, loss: 0.316090\n",
      "0m 5s\n",
      "Epoch 62/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.338109, dice: 0.162893, loss: 0.250501\n",
      "val: bce: 0.455609, dice: 0.208673, loss: 0.332141\n",
      "0m 5s\n",
      "Epoch 63/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.337111, dice: 0.166345, loss: 0.251728\n",
      "val: bce: 0.416052, dice: 0.187712, loss: 0.301882\n",
      "0m 5s\n",
      "Epoch 64/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.329996, dice: 0.159688, loss: 0.244842\n",
      "val: bce: 0.416050, dice: 0.190987, loss: 0.303518\n",
      "0m 5s\n",
      "Epoch 65/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.323773, dice: 0.159862, loss: 0.241817\n",
      "val: bce: 0.408624, dice: 0.179927, loss: 0.294275\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 66/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.321774, dice: 0.155779, loss: 0.238776\n",
      "val: bce: 0.423572, dice: 0.193852, loss: 0.308712\n",
      "0m 5s\n",
      "Epoch 67/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.315663, dice: 0.153745, loss: 0.234704\n",
      "val: bce: 0.438991, dice: 0.207468, loss: 0.323230\n",
      "0m 5s\n",
      "Epoch 68/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.309402, dice: 0.152960, loss: 0.231181\n",
      "val: bce: 0.408482, dice: 0.177850, loss: 0.293166\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 69/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.306667, dice: 0.148687, loss: 0.227677\n",
      "val: bce: 0.421269, dice: 0.189541, loss: 0.305405\n",
      "0m 5s\n",
      "Epoch 70/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.304052, dice: 0.150247, loss: 0.227150\n",
      "val: bce: 0.428119, dice: 0.202218, loss: 0.315168\n",
      "0m 5s\n",
      "Epoch 71/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.300436, dice: 0.146383, loss: 0.223410\n",
      "val: bce: 0.407646, dice: 0.184956, loss: 0.296301\n",
      "0m 5s\n",
      "Epoch 72/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.299941, dice: 0.148785, loss: 0.224363\n",
      "val: bce: 0.413506, dice: 0.179699, loss: 0.296603\n",
      "0m 5s\n",
      "Epoch 73/199\n",
      "----------\n",
      "LR 0.0003200000000000001\n",
      "train: bce: 0.299145, dice: 0.145305, loss: 0.222225\n",
      "val: bce: 0.413005, dice: 0.183395, loss: 0.298200\n",
      "0m 5s\n",
      "Epoch 74/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.290674, dice: 0.145076, loss: 0.217875\n",
      "val: bce: 0.417283, dice: 0.172547, loss: 0.294915\n",
      "0m 5s\n",
      "Epoch 75/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.288674, dice: 0.141085, loss: 0.214879\n",
      "val: bce: 0.406756, dice: 0.183987, loss: 0.295371\n",
      "0m 5s\n",
      "Epoch 76/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.285666, dice: 0.142344, loss: 0.214005\n",
      "val: bce: 0.430280, dice: 0.179709, loss: 0.304994\n",
      "0m 5s\n",
      "Epoch 77/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.281656, dice: 0.139357, loss: 0.210507\n",
      "val: bce: 0.412767, dice: 0.172473, loss: 0.292620\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 78/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.281640, dice: 0.139927, loss: 0.210783\n",
      "val: bce: 0.413760, dice: 0.181669, loss: 0.297714\n",
      "0m 5s\n",
      "Epoch 79/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.277829, dice: 0.137544, loss: 0.207687\n",
      "val: bce: 0.397021, dice: 0.172736, loss: 0.284879\n",
      "saving best model\n",
      "0m 5s\n",
      "Epoch 80/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.273581, dice: 0.137220, loss: 0.205400\n",
      "val: bce: 0.432081, dice: 0.175861, loss: 0.303971\n",
      "0m 5s\n",
      "Epoch 81/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.273893, dice: 0.135888, loss: 0.204890\n",
      "val: bce: 0.415918, dice: 0.179900, loss: 0.297909\n",
      "0m 5s\n",
      "Epoch 82/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.272124, dice: 0.136540, loss: 0.204332\n",
      "val: bce: 0.436292, dice: 0.185632, loss: 0.310962\n",
      "0m 5s\n",
      "Epoch 83/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.269600, dice: 0.134869, loss: 0.202235\n",
      "val: bce: 0.457752, dice: 0.188274, loss: 0.323013\n",
      "0m 5s\n",
      "Epoch 84/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.272155, dice: 0.135847, loss: 0.204001\n",
      "val: bce: 0.427727, dice: 0.187052, loss: 0.307389\n",
      "0m 5s\n",
      "Epoch 85/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.265060, dice: 0.133954, loss: 0.199507\n",
      "val: bce: 0.415288, dice: 0.170614, loss: 0.292951\n",
      "0m 5s\n",
      "Epoch 86/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.262922, dice: 0.131793, loss: 0.197357\n",
      "val: bce: 0.423865, dice: 0.178896, loss: 0.301381\n",
      "0m 5s\n",
      "Epoch 87/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.261627, dice: 0.133202, loss: 0.197415\n",
      "val: bce: 0.457499, dice: 0.172113, loss: 0.314806\n",
      "0m 5s\n",
      "Epoch 88/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.266475, dice: 0.132999, loss: 0.199737\n",
      "val: bce: 0.416039, dice: 0.172872, loss: 0.294455\n",
      "0m 5s\n",
      "Epoch 89/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.263050, dice: 0.133017, loss: 0.198034\n",
      "val: bce: 0.427763, dice: 0.168665, loss: 0.298214\n",
      "0m 5s\n",
      "Epoch 90/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.258518, dice: 0.130851, loss: 0.194684\n",
      "val: bce: 0.442411, dice: 0.177188, loss: 0.309800\n",
      "0m 5s\n",
      "Epoch 91/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.255199, dice: 0.130091, loss: 0.192645\n",
      "val: bce: 0.430356, dice: 0.170115, loss: 0.300236\n",
      "0m 5s\n",
      "Epoch 92/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.252526, dice: 0.128579, loss: 0.190553\n",
      "val: bce: 0.453494, dice: 0.166744, loss: 0.310119\n",
      "0m 5s\n",
      "Epoch 93/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.250983, dice: 0.127969, loss: 0.189476\n",
      "val: bce: 0.460384, dice: 0.173303, loss: 0.316843\n",
      "0m 5s\n",
      "Epoch 94/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.249170, dice: 0.127290, loss: 0.188230\n",
      "val: bce: 0.451569, dice: 0.175541, loss: 0.313555\n",
      "0m 5s\n",
      "Epoch 95/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.247653, dice: 0.126950, loss: 0.187301\n",
      "val: bce: 0.484873, dice: 0.172305, loss: 0.328589\n",
      "0m 5s\n",
      "Epoch 96/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.246916, dice: 0.126931, loss: 0.186924\n",
      "val: bce: 0.458709, dice: 0.170494, loss: 0.314601\n",
      "0m 5s\n",
      "Epoch 97/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.245013, dice: 0.125763, loss: 0.185388\n",
      "val: bce: 0.468675, dice: 0.175856, loss: 0.322265\n",
      "0m 5s\n",
      "Epoch 98/199\n",
      "----------\n",
      "LR 0.00025600000000000004\n",
      "train: bce: 0.245682, dice: 0.126371, loss: 0.186026\n",
      "val: bce: 0.473911, dice: 0.166572, loss: 0.320241\n",
      "0m 5s\n",
      "Epoch 99/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.247292, dice: 0.126318, loss: 0.186805\n",
      "val: bce: 0.470986, dice: 0.180117, loss: 0.325552\n",
      "0m 5s\n",
      "Epoch 100/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.244871, dice: 0.126320, loss: 0.185595\n",
      "val: bce: 0.455299, dice: 0.173575, loss: 0.314437\n",
      "0m 5s\n",
      "Epoch 101/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.241999, dice: 0.124931, loss: 0.183465\n",
      "val: bce: 0.489616, dice: 0.173740, loss: 0.331678\n",
      "0m 5s\n",
      "Epoch 102/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.242001, dice: 0.125210, loss: 0.183605\n",
      "val: bce: 0.490940, dice: 0.170172, loss: 0.330556\n",
      "0m 5s\n",
      "Epoch 103/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.240728, dice: 0.124041, loss: 0.182385\n",
      "val: bce: 0.462717, dice: 0.169459, loss: 0.316088\n",
      "0m 5s\n",
      "Epoch 104/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.240660, dice: 0.123513, loss: 0.182086\n",
      "val: bce: 0.487534, dice: 0.177046, loss: 0.332290\n",
      "0m 5s\n",
      "Epoch 105/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.238420, dice: 0.123999, loss: 0.181209\n",
      "val: bce: 0.514085, dice: 0.174807, loss: 0.344446\n",
      "0m 5s\n",
      "Epoch 106/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.00020480000000000004\n",
      "train: bce: 0.237996, dice: 0.122870, loss: 0.180433\n",
      "val: bce: 0.481726, dice: 0.171830, loss: 0.326778\n",
      "0m 5s\n",
      "Epoch 107/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.235165, dice: 0.122236, loss: 0.178700\n",
      "val: bce: 0.510347, dice: 0.171227, loss: 0.340787\n",
      "0m 5s\n",
      "Epoch 108/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.233859, dice: 0.121547, loss: 0.177703\n",
      "val: bce: 0.497794, dice: 0.170621, loss: 0.334207\n",
      "0m 5s\n",
      "Epoch 109/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.232346, dice: 0.121488, loss: 0.176917\n",
      "val: bce: 0.529983, dice: 0.176835, loss: 0.353409\n",
      "0m 5s\n",
      "Epoch 110/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.231418, dice: 0.120856, loss: 0.176137\n",
      "val: bce: 0.518000, dice: 0.170189, loss: 0.344095\n",
      "0m 5s\n",
      "Epoch 111/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.231162, dice: 0.120807, loss: 0.175985\n",
      "val: bce: 0.528361, dice: 0.166989, loss: 0.347675\n",
      "0m 5s\n",
      "Epoch 112/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.230764, dice: 0.120652, loss: 0.175708\n",
      "val: bce: 0.525081, dice: 0.171103, loss: 0.348092\n",
      "0m 5s\n",
      "Epoch 113/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.229451, dice: 0.119906, loss: 0.174678\n",
      "val: bce: 0.523679, dice: 0.170973, loss: 0.347326\n",
      "0m 5s\n",
      "Epoch 114/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.228100, dice: 0.119799, loss: 0.173950\n",
      "val: bce: 0.568279, dice: 0.172909, loss: 0.370594\n",
      "0m 5s\n",
      "Epoch 115/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.228556, dice: 0.119540, loss: 0.174048\n",
      "val: bce: 0.543505, dice: 0.168198, loss: 0.355851\n",
      "0m 5s\n",
      "Epoch 116/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.228087, dice: 0.119179, loss: 0.173633\n",
      "val: bce: 0.557585, dice: 0.171076, loss: 0.364331\n",
      "0m 5s\n",
      "Epoch 117/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.227569, dice: 0.119912, loss: 0.173740\n",
      "val: bce: 0.531477, dice: 0.165957, loss: 0.348717\n",
      "0m 5s\n",
      "Epoch 118/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.236916, dice: 0.122254, loss: 0.179585\n",
      "val: bce: 0.520104, dice: 0.174724, loss: 0.347414\n",
      "0m 5s\n",
      "Epoch 119/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.234549, dice: 0.122195, loss: 0.178372\n",
      "val: bce: 0.510974, dice: 0.169747, loss: 0.340361\n",
      "0m 5s\n",
      "Epoch 120/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.231184, dice: 0.120480, loss: 0.175832\n",
      "val: bce: 0.542275, dice: 0.169275, loss: 0.355775\n",
      "0m 5s\n",
      "Epoch 121/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.227001, dice: 0.119309, loss: 0.173155\n",
      "val: bce: 0.522776, dice: 0.166156, loss: 0.344466\n",
      "0m 5s\n",
      "Epoch 122/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.230901, dice: 0.120204, loss: 0.175552\n",
      "val: bce: 0.523772, dice: 0.161311, loss: 0.342541\n",
      "0m 5s\n",
      "Epoch 123/199\n",
      "----------\n",
      "LR 0.00020480000000000004\n",
      "train: bce: 0.230592, dice: 0.120522, loss: 0.175557\n",
      "val: bce: 0.525266, dice: 0.167414, loss: 0.346340\n",
      "0m 5s\n",
      "Epoch 124/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.231340, dice: 0.119825, loss: 0.175582\n",
      "val: bce: 0.542327, dice: 0.179200, loss: 0.360763\n",
      "0m 5s\n",
      "Epoch 125/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.225844, dice: 0.118631, loss: 0.172237\n",
      "val: bce: 0.522431, dice: 0.173885, loss: 0.348158\n",
      "0m 5s\n",
      "Epoch 126/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.222492, dice: 0.116954, loss: 0.169723\n",
      "val: bce: 0.583265, dice: 0.173362, loss: 0.378313\n",
      "0m 5s\n",
      "Epoch 127/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.220529, dice: 0.116290, loss: 0.168410\n",
      "val: bce: 0.567751, dice: 0.172150, loss: 0.369950\n",
      "0m 5s\n",
      "Epoch 128/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.219087, dice: 0.115642, loss: 0.167364\n",
      "val: bce: 0.561367, dice: 0.171211, loss: 0.366289\n",
      "0m 5s\n",
      "Epoch 129/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.218986, dice: 0.115737, loss: 0.167362\n",
      "val: bce: 0.573506, dice: 0.170220, loss: 0.371863\n",
      "0m 5s\n",
      "Epoch 130/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.219058, dice: 0.115816, loss: 0.167437\n",
      "val: bce: 0.587981, dice: 0.171488, loss: 0.379734\n",
      "0m 5s\n",
      "Epoch 131/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.217464, dice: 0.115192, loss: 0.166328\n",
      "val: bce: 0.600767, dice: 0.175887, loss: 0.388327\n",
      "0m 5s\n",
      "Epoch 132/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.216255, dice: 0.114665, loss: 0.165460\n",
      "val: bce: 0.604628, dice: 0.168884, loss: 0.386756\n",
      "0m 5s\n",
      "Epoch 133/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.214845, dice: 0.114126, loss: 0.164485\n",
      "val: bce: 0.614759, dice: 0.169126, loss: 0.391943\n",
      "0m 5s\n",
      "Epoch 134/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.214640, dice: 0.114189, loss: 0.164414\n",
      "val: bce: 0.622021, dice: 0.165795, loss: 0.393908\n",
      "0m 5s\n",
      "Epoch 135/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.215119, dice: 0.113568, loss: 0.164343\n",
      "val: bce: 0.631483, dice: 0.171460, loss: 0.401472\n",
      "0m 5s\n",
      "Epoch 136/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.217637, dice: 0.114942, loss: 0.166289\n",
      "val: bce: 0.661396, dice: 0.174846, loss: 0.418121\n",
      "0m 5s\n",
      "Epoch 137/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.214853, dice: 0.114282, loss: 0.164567\n",
      "val: bce: 0.631425, dice: 0.171962, loss: 0.401693\n",
      "0m 5s\n",
      "Epoch 138/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.213213, dice: 0.113176, loss: 0.163195\n",
      "val: bce: 0.629682, dice: 0.174345, loss: 0.402014\n",
      "0m 5s\n",
      "Epoch 139/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.213606, dice: 0.113596, loss: 0.163601\n",
      "val: bce: 0.603316, dice: 0.171070, loss: 0.387193\n",
      "0m 5s\n",
      "Epoch 140/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.212057, dice: 0.112764, loss: 0.162410\n",
      "val: bce: 0.647924, dice: 0.164108, loss: 0.406016\n",
      "0m 5s\n",
      "Epoch 141/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.213013, dice: 0.112678, loss: 0.162845\n",
      "val: bce: 0.627742, dice: 0.175610, loss: 0.401676\n",
      "0m 5s\n",
      "Epoch 142/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.212207, dice: 0.112941, loss: 0.162574\n",
      "val: bce: 0.688348, dice: 0.170765, loss: 0.429556\n",
      "0m 5s\n",
      "Epoch 143/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.213440, dice: 0.113045, loss: 0.163243\n",
      "val: bce: 0.627773, dice: 0.167787, loss: 0.397780\n",
      "0m 5s\n",
      "Epoch 144/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.213855, dice: 0.113839, loss: 0.163847\n",
      "val: bce: 0.630652, dice: 0.164863, loss: 0.397757\n",
      "0m 5s\n",
      "Epoch 145/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.213866, dice: 0.112914, loss: 0.163390\n",
      "val: bce: 0.631841, dice: 0.165864, loss: 0.398852\n",
      "0m 5s\n",
      "Epoch 146/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.216572, dice: 0.114491, loss: 0.165532\n",
      "val: bce: 0.597110, dice: 0.172501, loss: 0.384805\n",
      "0m 5s\n",
      "Epoch 147/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.216714, dice: 0.113442, loss: 0.165078\n",
      "val: bce: 0.602184, dice: 0.164987, loss: 0.383586\n",
      "0m 5s\n",
      "Epoch 148/199\n",
      "----------\n",
      "LR 0.00016384000000000006\n",
      "train: bce: 0.214870, dice: 0.114096, loss: 0.164483\n",
      "val: bce: 0.620861, dice: 0.168357, loss: 0.394609\n",
      "0m 5s\n",
      "Epoch 149/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.213987, dice: 0.113037, loss: 0.163512\n",
      "val: bce: 0.644387, dice: 0.174792, loss: 0.409589\n",
      "0m 5s\n",
      "Epoch 150/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.208648, dice: 0.111231, loss: 0.159940\n",
      "val: bce: 0.614376, dice: 0.166876, loss: 0.390626\n",
      "0m 5s\n",
      "Epoch 151/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.207109, dice: 0.110487, loss: 0.158798\n",
      "val: bce: 0.646673, dice: 0.167663, loss: 0.407168\n",
      "0m 5s\n",
      "Epoch 152/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.205372, dice: 0.109796, loss: 0.157584\n",
      "val: bce: 0.689532, dice: 0.172411, loss: 0.430971\n",
      "0m 5s\n",
      "Epoch 153/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.204750, dice: 0.109286, loss: 0.157018\n",
      "val: bce: 0.721555, dice: 0.180219, loss: 0.450887\n",
      "0m 5s\n",
      "Epoch 154/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.204848, dice: 0.109292, loss: 0.157070\n",
      "val: bce: 0.704978, dice: 0.168646, loss: 0.436812\n",
      "0m 5s\n",
      "Epoch 155/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.203324, dice: 0.108922, loss: 0.156123\n",
      "val: bce: 0.708933, dice: 0.169202, loss: 0.439067\n",
      "0m 5s\n",
      "Epoch 156/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.206403, dice: 0.109793, loss: 0.158098\n",
      "val: bce: 0.688774, dice: 0.168027, loss: 0.428401\n",
      "0m 5s\n",
      "Epoch 157/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.00013107200000000006\n",
      "train: bce: 0.204328, dice: 0.109132, loss: 0.156730\n",
      "val: bce: 0.672387, dice: 0.164839, loss: 0.418613\n",
      "0m 5s\n",
      "Epoch 158/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.204502, dice: 0.109184, loss: 0.156843\n",
      "val: bce: 0.707690, dice: 0.170521, loss: 0.439106\n",
      "0m 5s\n",
      "Epoch 159/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.201885, dice: 0.108026, loss: 0.154956\n",
      "val: bce: 0.765396, dice: 0.173465, loss: 0.469431\n",
      "0m 5s\n",
      "Epoch 160/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.200428, dice: 0.107649, loss: 0.154038\n",
      "val: bce: 0.736628, dice: 0.172894, loss: 0.454761\n",
      "0m 5s\n",
      "Epoch 161/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.200651, dice: 0.107275, loss: 0.153963\n",
      "val: bce: 0.713735, dice: 0.174908, loss: 0.444321\n",
      "0m 5s\n",
      "Epoch 162/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.200342, dice: 0.107066, loss: 0.153704\n",
      "val: bce: 0.755340, dice: 0.174265, loss: 0.464803\n",
      "0m 5s\n",
      "Epoch 163/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.199242, dice: 0.107003, loss: 0.153123\n",
      "val: bce: 0.745461, dice: 0.165608, loss: 0.455534\n",
      "0m 5s\n",
      "Epoch 164/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.197973, dice: 0.105951, loss: 0.151962\n",
      "val: bce: 0.763594, dice: 0.175082, loss: 0.469338\n",
      "0m 5s\n",
      "Epoch 165/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.197108, dice: 0.106055, loss: 0.151582\n",
      "val: bce: 0.810820, dice: 0.172432, loss: 0.491626\n",
      "0m 5s\n",
      "Epoch 166/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.197060, dice: 0.105618, loss: 0.151339\n",
      "val: bce: 0.801218, dice: 0.168970, loss: 0.485094\n",
      "0m 5s\n",
      "Epoch 167/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.196801, dice: 0.105809, loss: 0.151305\n",
      "val: bce: 0.807541, dice: 0.172995, loss: 0.490268\n",
      "0m 5s\n",
      "Epoch 168/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.196874, dice: 0.105342, loss: 0.151108\n",
      "val: bce: 0.824288, dice: 0.170332, loss: 0.497310\n",
      "0m 5s\n",
      "Epoch 169/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.196496, dice: 0.105234, loss: 0.150865\n",
      "val: bce: 0.849692, dice: 0.171716, loss: 0.510704\n",
      "0m 5s\n",
      "Epoch 170/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.195889, dice: 0.105197, loss: 0.150543\n",
      "val: bce: 0.829361, dice: 0.171787, loss: 0.500574\n",
      "0m 5s\n",
      "Epoch 171/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.197095, dice: 0.105272, loss: 0.151183\n",
      "val: bce: 0.760592, dice: 0.170022, loss: 0.465307\n",
      "0m 5s\n",
      "Epoch 172/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.198319, dice: 0.105284, loss: 0.151802\n",
      "val: bce: 0.756793, dice: 0.173131, loss: 0.464962\n",
      "0m 5s\n",
      "Epoch 173/199\n",
      "----------\n",
      "LR 0.00013107200000000006\n",
      "train: bce: 0.199975, dice: 0.106736, loss: 0.153355\n",
      "val: bce: 0.861360, dice: 0.167046, loss: 0.514203\n",
      "0m 5s\n",
      "Epoch 174/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.201549, dice: 0.106521, loss: 0.154035\n",
      "val: bce: 0.686424, dice: 0.178695, loss: 0.432559\n",
      "0m 5s\n",
      "Epoch 175/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.200987, dice: 0.107233, loss: 0.154110\n",
      "val: bce: 0.768838, dice: 0.164029, loss: 0.466433\n",
      "0m 5s\n",
      "Epoch 176/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.197286, dice: 0.105455, loss: 0.151371\n",
      "val: bce: 0.753710, dice: 0.172078, loss: 0.462894\n",
      "0m 5s\n",
      "Epoch 177/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.198232, dice: 0.106077, loss: 0.152155\n",
      "val: bce: 0.764684, dice: 0.169991, loss: 0.467338\n",
      "0m 5s\n",
      "Epoch 178/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.195235, dice: 0.104463, loss: 0.149849\n",
      "val: bce: 0.804336, dice: 0.171065, loss: 0.487700\n",
      "0m 5s\n",
      "Epoch 179/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.192515, dice: 0.103728, loss: 0.148122\n",
      "val: bce: 0.806457, dice: 0.170840, loss: 0.488648\n",
      "0m 5s\n",
      "Epoch 180/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.191469, dice: 0.102975, loss: 0.147222\n",
      "val: bce: 0.874411, dice: 0.174657, loss: 0.524534\n",
      "0m 5s\n",
      "Epoch 181/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.190681, dice: 0.102767, loss: 0.146724\n",
      "val: bce: 0.841218, dice: 0.168625, loss: 0.504922\n",
      "0m 5s\n",
      "Epoch 182/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.190246, dice: 0.101832, loss: 0.146039\n",
      "val: bce: 0.793919, dice: 0.171541, loss: 0.482730\n",
      "0m 5s\n",
      "Epoch 183/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.192128, dice: 0.102732, loss: 0.147430\n",
      "val: bce: 0.827132, dice: 0.173176, loss: 0.500154\n",
      "0m 5s\n",
      "Epoch 184/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.189938, dice: 0.102194, loss: 0.146066\n",
      "val: bce: 0.869363, dice: 0.170646, loss: 0.520005\n",
      "0m 5s\n",
      "Epoch 185/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.189229, dice: 0.101642, loss: 0.145435\n",
      "val: bce: 0.929702, dice: 0.175394, loss: 0.552548\n",
      "0m 5s\n",
      "Epoch 186/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.191133, dice: 0.102183, loss: 0.146658\n",
      "val: bce: 0.902067, dice: 0.179219, loss: 0.540643\n",
      "0m 5s\n",
      "Epoch 187/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.191804, dice: 0.102537, loss: 0.147170\n",
      "val: bce: 0.937586, dice: 0.181927, loss: 0.559757\n",
      "0m 5s\n",
      "Epoch 188/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.193385, dice: 0.103276, loss: 0.148331\n",
      "val: bce: 0.857621, dice: 0.167590, loss: 0.512606\n",
      "0m 5s\n",
      "Epoch 189/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.189973, dice: 0.102177, loss: 0.146075\n",
      "val: bce: 0.850416, dice: 0.166894, loss: 0.508655\n",
      "0m 5s\n",
      "Epoch 190/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.190126, dice: 0.101556, loss: 0.145841\n",
      "val: bce: 0.862793, dice: 0.171158, loss: 0.516976\n",
      "0m 5s\n",
      "Epoch 191/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.187700, dice: 0.101019, loss: 0.144359\n",
      "val: bce: 0.917443, dice: 0.172240, loss: 0.544842\n",
      "0m 5s\n",
      "Epoch 192/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.186178, dice: 0.100095, loss: 0.143136\n",
      "val: bce: 0.918192, dice: 0.171623, loss: 0.544907\n",
      "0m 5s\n",
      "Epoch 193/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.186163, dice: 0.099906, loss: 0.143034\n",
      "val: bce: 0.910331, dice: 0.175846, loss: 0.543089\n",
      "0m 5s\n",
      "Epoch 194/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.186200, dice: 0.099750, loss: 0.142975\n",
      "val: bce: 0.927314, dice: 0.170652, loss: 0.548983\n",
      "0m 5s\n",
      "Epoch 195/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.185931, dice: 0.100070, loss: 0.143001\n",
      "val: bce: 0.964124, dice: 0.168231, loss: 0.566177\n",
      "0m 5s\n",
      "Epoch 196/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.187270, dice: 0.100066, loss: 0.143668\n",
      "val: bce: 0.894295, dice: 0.172107, loss: 0.533201\n",
      "0m 5s\n",
      "Epoch 197/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.185609, dice: 0.099624, loss: 0.142616\n",
      "val: bce: 1.041550, dice: 0.169185, loss: 0.605367\n",
      "0m 5s\n",
      "Epoch 198/199\n",
      "----------\n",
      "LR 0.00010485760000000004\n",
      "train: bce: 0.186451, dice: 0.099369, loss: 0.142910\n",
      "val: bce: 0.959928, dice: 0.170436, loss: 0.565182\n",
      "0m 5s\n",
      "Epoch 199/199\n",
      "----------\n",
      "LR 8.388608000000005e-05\n",
      "train: bce: 0.188060, dice: 0.100646, loss: 0.144353\n",
      "val: bce: 0.898697, dice: 0.169572, loss: 0.534134\n",
      "0m 5s\n",
      "Best val loss: 0.284879\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "import pytorch_unet\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 1\n",
    "\n",
    "model = pytorch_unet.UNet(num_class).to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.8)\n",
    "\n",
    "model,optimizer_ft, exp_lr_scheduler = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"vis_vl0.28_13_200_epoch.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randint(low = 0, high = 3, size = (5,5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 1, 2, 2],\n",
       "       [0, 0, 1, 1, 2],\n",
       "       [2, 0, 1, 0, 0],\n",
       "       [1, 2, 2, 1, 2],\n",
       "       [2, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr =  np.logical_or( arr == 1, arr ==2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True],\n",
       "       [False, False,  True,  True,  True],\n",
       "       [ True, False,  True, False, False],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True, False,  True, False,  True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(test_list)):\n",
    "    mask = np.array(Image.open(f'./../SIH/INSAT_Cloud_Labels/CROPPED_TIFF/CMK_Cropped_{tir_to_num[test_list[idx]]}.tif'))[:,:1072]\n",
    "    #image = np.array(Image.open(f'./INSAT3D_TIR1_India/{test_list[idx]}'))[:,:1072]\n",
    "    check = mask==9\n",
    "    if check.any():\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.load('best_unet_model_visband_val_loss_0.25_1_labelclass_150_epoch.ckpt')\n",
    "#model = model.to(device)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3DIMG_07NOV2019_0600_L1C_SGP.tif', '3DIMG_07NOV2019_0800_L1C_SGP.tif', '3DIMG_07NOV2019_0830_L1C_SGP.tif', '3DIMG_07NOV2019_0400_L1C_SGP.tif', '3DIMG_07NOV2019_1000_L1C_SGP.tif', '3DIMG_07NOV2019_0700_L1C_SGP.tif', '3DIMG_07NOV2019_0930_L1C_SGP.tif', '3DIMG_07NOV2019_0959_L1C_SGP.tif', '3DIMG_07NOV2019_0530_L1C_SGP.tif']\n"
     ]
    }
   ],
   "source": [
    "image = np.array(Image.open(f'./INSAT3D_VIS_India/3DIMG_07NOV2019_0600_L1C_SGP_vis.tif'))[:,:1072]            \n",
    "print(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = torch.from_numpy(image)\n",
    "img_tensor = img_tensor.view((1,1,984,1072))\n",
    "\n",
    "img_tensor = img_tensor.to(device,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 984, 1072])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = out.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(984, 1072)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = np.squeeze(output)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img = Image.fromarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img.save(\"out_vis_6000.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
